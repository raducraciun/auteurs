\chapter*{Conclusion}

Ce projet de fin d'études fut une occasion de découvrir en détail une science jeune et prometteuse qu'est celle de l'apprentissage artificiel. Bien évidemment, ceci s'est accompagné de difficultés inhérentes : le sujet est vaste et très pointu. Même si l'on s'est restreint à l'apprentissage profond, nous n'avons pas eu le choix que de ne pas traiter certains sujets très intéressants. Typiquement, nous n'avons pas traité des grandes architectures de réseaux de neurones et de leurs particularités, ni des bibliothèques destinées à l'apprentissage et leurs optimisations.

La question de départ, à savoir si les auteurs sont reconnaissables par leur style d'écriture, reste ouverte, car la notion de style est ambigüe. Dans l'absolu, cela a été fait par OpenAI, allant même jusqu'à imiter parfaitement le style d'une journaliste. Cela ne nous donne pas beaucoup plus de pistes pour savoir \textit{comment} le faire. On revient donc aux interrogations de l'introduction : que veut dire créer ou imiter ? Faut-il passer par une représentation sémantique des données pour gagner en assurance ?

Comme nous l'avons vu, les promesses de l'IA sont nombreuses, mais les défis pour les tenir sont plus nombreux encore. De même, le fait que l'intelligence artificielle acquiert un grand pouvoir, ou du moins une place dans nos vies quotidiennes, s'accompagne de risques. Or, on l'a vu, peu de sciences sont aussi imprévisibles que celle-ci. Il est donc illusoire et dangereux de négliger les risques d'une éventuelle intelligence artificielle de niveau humain. En particulier, il est indispensable de concevoir une morale robuste pour une intelligence artificielle avancée. Nous savons maintenant que les réseaux de neurones sont de formidables machines à optimiser des fonctions. Mais cela ne suffit pas, car, pour citer Charles Goodhart, "Dès qu'une mesure devient l'objectif, elle cesse d'être une bonne mesure". 